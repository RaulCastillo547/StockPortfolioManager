{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "213f4bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datetime import time\n",
    "from datetime import timedelta\n",
    "import json\n",
    "import os\n",
    "from urllib import request\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "99c15f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n",
      "Program Paused\n",
      "Program Continued\n"
     ]
    }
   ],
   "source": [
    "def pause(seconds):\n",
    "    tmp = datetime.now()\n",
    "    while (datetime.now() - tmp).total_seconds() <= seconds:\n",
    "        pass\n",
    "\n",
    "class PortfolioMaster():\n",
    "    # Sets up vartiables for the class and creates \n",
    "    # the files storing stock data, if not present\n",
    "    def __init__(self, folder_name):\n",
    "        self.folder_name = folder_name\n",
    "        self.hold_url = folder_name + '/Transactions on Hold.csv'\n",
    "        self.post_url = folder_name + '/Posted Transactions.csv'\n",
    "        self.overview_url = folder_name + '/Overview.csv'\n",
    "        self.meta_url = folder_name + '/Meta.csv'\n",
    "\n",
    "        self.api_key = os.environ.get('Stock_API_key')\n",
    "\n",
    "        if folder_name not in os.listdir():\n",
    "            os.makedirs(folder_name)\n",
    "\n",
    "        if 'Overview.csv' not in os.listdir(folder_name):\n",
    "            overview_df = pd.DataFrame({'Stock': [], 'Quantity': [], \n",
    "                                        'Amount Invested': [], \n",
    "                                        'Invested per Share': [], \n",
    "                                        'Current Worth': [], 'Price per Share': [], \n",
    "                                        'Current Profit/Loss': [], 'Last Updated': []})\n",
    "            overview_df.to_csv(self.overview_url, index=False, mode='w+')\n",
    "\n",
    "        if 'Transactions on Hold.csv' not in os.listdir(folder_name):\n",
    "            hold_df = pd.DataFrame({'Date': [], 'Stock': [], 'Quantity Moved': []})\n",
    "            hold_df.to_csv(self.hold_url, index=False, mode='w+')\n",
    "        \n",
    "        if 'Posted Transactions.csv' not in os.listdir(folder_name):\n",
    "            post_df = pd.DataFrame({'Time Posted': [], 'Stock': [],\n",
    "                                    'Quantity Moved': [], 'Price Per Share': [],\n",
    "                                    'Amount Moved': [], 'Last Updated': []})\n",
    "            post_df.to_csv(self.post_url, index=False, mode='w+')\n",
    "\n",
    "        if 'Meta.csv' not in os.listdir(folder_name):\n",
    "            meta_df = pd.DataFrame({'Item': ['Cash'], \n",
    "                                    'Quantity': [0]})\n",
    "            meta_df.to_csv(self.meta_url, index=False, mode='w+')\n",
    "\n",
    "    def call_order(self, symbol, quantity):\n",
    "        # Begins a transaction that is placed in the Hold Transactions csv\n",
    "        # before load_orders() is called\n",
    "\n",
    "        hold_df = pd.read_csv(self.hold_url)\n",
    "        add_on_df = pd.DataFrame({'Date': [datetime.now().strftime('%Y-%m-%d %H:%M:%S')], \n",
    "                                    'Stock': [symbol],'Quantity Moved':[quantity]})\n",
    "        hold_df = pd.concat([hold_df, add_on_df], axis=0, ignore_index=True)\n",
    "        hold_df.to_csv(self.hold_url, index=False,  mode='w')\n",
    "\n",
    "    def load_orders(self):\n",
    "        # Brings the transactions listed on the Hold Transactions csv\n",
    "        # onto the post transactions csv\n",
    "\n",
    "        current = datetime.now() - timedelta(days=1)\n",
    "        hold_df = pd.read_csv(self.hold_url)\n",
    "\n",
    "        hold_extract = {'Time Posted': [], 'Stock': [], \n",
    "                        'Quantity Moved': [], 'Price Per Share': [], \n",
    "                        'Amount Moved': [], 'Last Updated': []}\n",
    "        index_removeable = []\n",
    "\n",
    "        for index in hold_df.index:\n",
    "            if (self.count_total_daily_calls()) > 500:\n",
    "                print('Exceeded API Cap of 500 Calls per Day')\n",
    "                break\n",
    "            \n",
    "            print(index + 1 + self.count_minute_calls())\n",
    "\n",
    "            if (index + 1 + self.count_minute_calls()) % 5 == 0:\n",
    "                print('Program Paused')\n",
    "                pause(65)\n",
    "                print('Program Continued')\n",
    "\n",
    "            # Extract data from Hold Transactions csv\n",
    "            stock = hold_df['Stock'][index]\n",
    "            quantity = float(hold_df['Quantity Moved'][index])\n",
    "            last_updated = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            # Extract data from JSON with current stock information\n",
    "            url_link = (r\"https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol={}&interval=15min&apikey={}\".format(stock, self.api_key))\n",
    "            url_data = request.urlopen(url_link)\n",
    "            raw_data = url_data.read()\n",
    "            json_data = json.loads(raw_data)\n",
    "            \n",
    "            # Parsing through date data from JSON and associated price per share data\n",
    "            dates = pd.DataFrame(json_data['Time Series (15min)']).transpose().reset_index()\n",
    "            dates = pd.to_datetime(dates['index']).dt.date\n",
    "            dates = pd.DataFrame(json_data['Time Series (15min)'])\n",
    "            dates = pd.to_datetime(dates.transpose().reset_index()['index'])\n",
    "\n",
    "            if current.date() not in dates:\n",
    "                stock_time = (datetime.strptime(json_data['Meta Data']['3. Last Refreshed'], '%Y-%m-%d %H:%M:%S').replace(hour=16, minute=0, second=0, microsecond=0)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                price_per_share = float(json_data['Time Series (15min)'][stock_time]['4. close'])\n",
    "            elif (time(hour = 9, minute=30) <= current.time()) and (current.time() <= time(hour=16)):\n",
    "                stock_time = json_data['Meta Data']['3. Last Refreshed']\n",
    "                price_per_share = float(json_data['Time Series (15min)'][stock_time]['4. close'])\n",
    "            elif (time.min <= current.time()) and (current.time() < time(hour=9, minute=30)): \n",
    "                stock_time = (current.replace(hour=16, minute=0, second=0, microsecond=0) - timedelta(days=1)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                price_per_share = float(json_data['Time Series (15min)'][stock_time]['4. close'])\n",
    "            elif (time(hour=16) <= current.time()) and (current.time() < time.max):\n",
    "                stock_time = current.replace(hour=16, minute=0, second=0, microsecond=0).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                price_per_share = float(json_data['Time Series (15min)'][stock_time]['4. close'])\n",
    "\n",
    "            meta_df = pd.read_csv(self.meta_url, index_col='Item')\n",
    "            if (meta_df['Quantity']['Cash'] - price_per_share*quantity) >= 0:\n",
    "                self.add_remove_cash(-1*price_per_share*quantity)\n",
    "            else:\n",
    "                print('Not Enough Money to Invest')\n",
    "                pass\n",
    "\n",
    "            # Summarize Data\n",
    "            hold_extract['Time Posted'] += [stock_time]\n",
    "            hold_extract['Stock'] += [stock]\n",
    "            hold_extract['Quantity Moved'] += [quantity]\n",
    "            hold_extract['Price Per Share'] += [price_per_share]\n",
    "            hold_extract['Amount Moved'] += [(price_per_share*10000)*quantity/10000]\n",
    "            hold_extract['Last Updated'] += [last_updated]\n",
    "\n",
    "            # Add Index\n",
    "            index_removeable += [index]\n",
    "\n",
    "        # Add to Posted Transactions\n",
    "        post_df = pd.read_csv(self.post_url)\n",
    "        addon_df = pd.DataFrame(hold_extract)\n",
    "        post_df = pd.concat([post_df, addon_df], axis=0, ignore_index=True)\n",
    "        post_df.to_csv(self.post_url, index=False, mode='w')\n",
    "        \n",
    "        # Remove Transactions That Were Posted\n",
    "        hold_df.drop(index_removeable, inplace=True)\n",
    "        hold_df.to_csv(self.hold_url, index=False, mode='w')\n",
    "\n",
    "    def update(self):\n",
    "        # Summarizes the data on the Posted Transactions csv \n",
    "        # onto the Overview csv\n",
    "        post_df = pd.read_csv(self.post_url)\n",
    "        post_dfgb = post_df.groupby('Stock')\n",
    "\n",
    "        output = {'Stock': [], 'Quantity': [], \n",
    "                    'Amount Invested': [], 'Invested per Share': [], \n",
    "                    'Current Worth': [], 'Price per Share': [], \n",
    "                    'Current Profit/Loss': [], 'Last Updated': []}\n",
    "\n",
    "        for index, group in enumerate(post_dfgb.groups):\n",
    "            if (self.count_total_daily_calls()) >= 500:\n",
    "                print('Exceeded API Cap of 500 Calls per Day')\n",
    "                break\n",
    "            \n",
    "            print(self.count_minute_calls())\n",
    "\n",
    "            if (index+1 + self.count_minute_calls()) % 5 == 0:\n",
    "                print('Program Paused')\n",
    "                pause(65)\n",
    "                print('Program Continued')\n",
    "\n",
    "            df_stock = post_dfgb.get_group(group)\n",
    "\n",
    "            # Extract from Post Transactions CSV\n",
    "            stock = group\n",
    "            quantity = df_stock['Quantity Moved'].astype(float).sum()\n",
    "            amount_invested = df_stock['Amount Moved'].astype(float).sum()\n",
    "            invested_per_share = (df_stock['Amount Moved'].astype(float).sum() \n",
    "                                    / df_stock['Quantity Moved'].astype(float).sum())\n",
    "\n",
    "            # Extract data from JSON with current stock information\n",
    "            url_link = (r\"https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol={}&interval=15min&apikey={}\".format(group, self.api_key))\n",
    "            url_data = request.urlopen(url_link)\n",
    "            raw_data = url_data.read()\n",
    "            json_data = json.loads(raw_data)\n",
    "            \n",
    "            current = datetime.now() - timedelta(days=1)\n",
    "\n",
    "            dates = pd.DataFrame(json_data['Time Series (15min)']).transpose().reset_index()\n",
    "            dates = pd.to_datetime(dates['index']).dt.date\n",
    "\n",
    "            # Parsing through date data from JSON and associated price per share data\n",
    "            if current.date() not in dates:\n",
    "                stock_time = (datetime.strptime(json_data['Meta Data']['3. Last Refreshed'], '%Y-%m-%d %H:%M:%S').replace(hour=16, minute=0, second=0, microsecond=0)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                price_per_share = float(json_data['Time Series (15min)'][stock_time]['4. close'])\n",
    "            elif (time(hour = 9, minute=30) <= current.time()) and (current.time() <= time(hour=16)):\n",
    "                stock_time = json_data['Meta Data']['3. Last Refreshed']\n",
    "                price_per_share = float(json_data['Time Series (15min)'][stock_time]['4. close'])\n",
    "            elif (time.min <= current.time()) and (current.time() < time(hour=9, minute=30)): \n",
    "                stock_time = (current.replace(hour=16, minute=0, second=0, microsecond=0) - timedelta(days=1)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                price_per_share = float(json_data['Time Series (15min)'][stock_time]['4. close'])\n",
    "            elif (time(hour=16) <= current.time()) and (current.time() < time.max):\n",
    "                stock_time = current.replace(hour=16, minute=0, second=0, microsecond=0).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                price_per_share = float(json_data['Time Series (15min)'][stock_time]['4. close'])\n",
    "            \n",
    "            current_worth = (price_per_share)*df_stock['Quantity Moved'].astype(float).sum()\n",
    "            profit_loss = (price_per_share)*df_stock['Quantity Moved'].astype(float).sum() - df_stock['Amount Moved'].astype(float).sum()\n",
    "\n",
    "            # Read overview csv\n",
    "            tmp_overview_df = pd.read_csv(self.overview_url)\n",
    "\n",
    "            if tmp_overview_df.size > 0: \n",
    "                tmp_overview_df[['Post_#', 'Posting Dates']] = tmp_overview_df['Last Updated'].apply(lambda x: pd.Series(str(x).split('_')))\n",
    "                tmp_overview_df['Posting Dates'] = tmp_overview_df['Posting Dates'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S').date())\n",
    "            tmp_overview_df.set_index('Stock', inplace=True)\n",
    "\n",
    "            try:\n",
    "                if tmp_overview_df.loc[stock, 'Posting Dates'] == datetime.now().date():\n",
    "                    count = int(tmp_overview_df.loc[stock, 'Post_#']) + 1\n",
    "                else:\n",
    "                    count = 1\n",
    "            except:\n",
    "                count = 1\n",
    "\n",
    "            # Summarize Data\n",
    "            output['Stock'] += [stock]\n",
    "            output['Quantity'] += [quantity]\n",
    "            output['Amount Invested'] += [amount_invested]\n",
    "            output['Invested per Share'] += [invested_per_share]\n",
    "            output['Price per Share'] += [price_per_share]\n",
    "            output['Current Worth'] += [current_worth]\n",
    "            output['Current Profit/Loss'] += [profit_loss]\n",
    "            output['Last Updated'] += [str(count) + '_' + str(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))]\n",
    "        \n",
    "        # Apply extracted data onto Overview csv\n",
    "        overview_df = pd.read_csv(self.overview_url)\n",
    "        \n",
    "        if overview_df.size > 0:\n",
    "            overview_df.set_index('Stock', inplace=True)\n",
    "            for stock in output['Stock']:\n",
    "                add_on = pd.DataFrame(output).set_index('Stock')\n",
    "                if stock in overview_df.index:\n",
    "                    overview_df.replace(overview_df.loc[stock], add_on.loc[stock], inplace=True)\n",
    "                elif stock not in overview_df.index:\n",
    "                    overview_df = pd.concat([overview_df, add_on], axis=0, ignore_index=False)\n",
    "            overview_df.reset_index(col_level=0, inplace=True)\n",
    "        else:\n",
    "            overview_df = pd.DataFrame(output)\n",
    "\n",
    "        overview_df.to_csv(self.overview_url, index=False, mode='w')\n",
    "\n",
    "    def table(self):\n",
    "        overview_df = pd.read_csv(self.overview_url, index_col='Stock')\n",
    "        overview_df['Last Updated'] = overview_df['Last Updated'].apply(lambda x: str(x).removeprefix(str(x)[:str(x).find('_')+1]))\n",
    "        print(overview_df)\n",
    "\n",
    "    def count_total_daily_calls(self):\n",
    "        current_date = datetime.now().date()\n",
    "        count = 0\n",
    "\n",
    "        overview_df = pd.read_csv(self.overview_url)\n",
    "        if overview_df.size > 0: \n",
    "            overview_df[['Post_#', 'Posting Dates']] = overview_df['Last Updated'].apply(lambda x: pd.Series(str(x).split('_')))\n",
    "\n",
    "            for index in overview_df.index:\n",
    "                if current_date.strftime('%Y-%m-%d') == datetime.strptime(overview_df.loc[index, 'Posting Dates'], '%Y-%m-%d %H:%M:%S').strftime('%Y-%m-%d'):\n",
    "                    count += pd.to_numeric(overview_df.loc[index, 'Post_#'])\n",
    "\n",
    "        post_df = pd.read_csv(self.post_url)\n",
    "        post_df = pd.to_datetime(post_df['Last Updated']).dt.date\n",
    "\n",
    "        for i in post_df:\n",
    "            if i == current_date:\n",
    "                count += 1\n",
    "\n",
    "        return count\n",
    "\n",
    "    def count_minute_calls(self):\n",
    "        count = 0\n",
    "\n",
    "        # Go through Overview csv\n",
    "        overview_df = pd.read_csv(self.overview_url)\n",
    "        if overview_df.size > 0:\n",
    "            overview_df[['Post_#', 'Dates']] = overview_df['Last Updated'].apply(lambda x: pd.Series(str(x).split('_')))\n",
    "            overview_df['Dates'] = overview_df['Dates'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S').replace(second=0, microsecond=0))\n",
    "\n",
    "            for date in overview_df['Dates']:\n",
    "                if date == datetime.now().replace(second=0, microsecond=0):\n",
    "                    count += 1\n",
    "        \n",
    "        # Go through Posted Transactions csv\n",
    "        post_df = pd.read_csv(self.post_url)\n",
    "        if post_df.size > 0:\n",
    "            post_df['Last Updated'] = post_df['Last Updated'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S').replace(second=0, microsecond=0))\n",
    "            for date in post_df['Last Updated']:\n",
    "                if date == datetime.now().replace(second=0, microsecond=0):\n",
    "                    count += 1\n",
    "\n",
    "        return count\n",
    "\n",
    "    def add_remove_cash(self, change):\n",
    "        meta_df = pd.read_csv(self.meta_url, index_col='Item')\n",
    "\n",
    "        cash = float(meta_df['Quantity']['Cash']) + change\n",
    "        meta_df.loc['Cash', 'Quantity'] = cash\n",
    "        meta_df.reset_index(col_level=0, inplace=True)\n",
    "\n",
    "        meta_df.to_csv(self.meta_url, index=False, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1929cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Timestamp('2023-03-15 20:00:00'),\n",
       " Timestamp('2023-03-15 19:45:00'),\n",
       " Timestamp('2023-03-15 19:30:00'),\n",
       " Timestamp('2023-03-15 18:45:00'),\n",
       " Timestamp('2023-03-15 18:15:00'),\n",
       " Timestamp('2023-03-15 17:30:00'),\n",
       " Timestamp('2023-03-15 17:15:00'),\n",
       " Timestamp('2023-03-15 17:00:00'),\n",
       " Timestamp('2023-03-15 16:30:00'),\n",
       " Timestamp('2023-03-15 16:15:00'),\n",
       " Timestamp('2023-03-15 16:00:00'),\n",
       " Timestamp('2023-03-15 15:45:00'),\n",
       " Timestamp('2023-03-15 15:30:00'),\n",
       " Timestamp('2023-03-15 15:15:00'),\n",
       " Timestamp('2023-03-15 15:00:00'),\n",
       " Timestamp('2023-03-15 14:45:00'),\n",
       " Timestamp('2023-03-15 14:30:00'),\n",
       " Timestamp('2023-03-15 14:15:00'),\n",
       " Timestamp('2023-03-15 14:00:00'),\n",
       " Timestamp('2023-03-15 13:45:00'),\n",
       " Timestamp('2023-03-15 13:30:00'),\n",
       " Timestamp('2023-03-15 13:15:00'),\n",
       " Timestamp('2023-03-15 13:00:00'),\n",
       " Timestamp('2023-03-15 12:45:00'),\n",
       " Timestamp('2023-03-15 12:30:00'),\n",
       " Timestamp('2023-03-15 12:15:00'),\n",
       " Timestamp('2023-03-15 12:00:00'),\n",
       " Timestamp('2023-03-15 11:45:00'),\n",
       " Timestamp('2023-03-15 11:30:00'),\n",
       " Timestamp('2023-03-15 11:15:00'),\n",
       " Timestamp('2023-03-15 11:00:00'),\n",
       " Timestamp('2023-03-15 10:45:00'),\n",
       " Timestamp('2023-03-15 10:30:00'),\n",
       " Timestamp('2023-03-15 10:15:00'),\n",
       " Timestamp('2023-03-15 10:00:00'),\n",
       " Timestamp('2023-03-15 09:45:00'),\n",
       " Timestamp('2023-03-15 09:15:00'),\n",
       " Timestamp('2023-03-15 08:45:00'),\n",
       " Timestamp('2023-03-15 08:30:00'),\n",
       " Timestamp('2023-03-15 08:15:00'),\n",
       " Timestamp('2023-03-15 07:30:00'),\n",
       " Timestamp('2023-03-15 07:15:00'),\n",
       " Timestamp('2023-03-14 19:00:00'),\n",
       " Timestamp('2023-03-14 17:45:00'),\n",
       " Timestamp('2023-03-14 17:00:00'),\n",
       " Timestamp('2023-03-14 16:30:00'),\n",
       " Timestamp('2023-03-14 16:15:00'),\n",
       " Timestamp('2023-03-14 16:00:00'),\n",
       " Timestamp('2023-03-14 15:45:00'),\n",
       " Timestamp('2023-03-14 15:30:00'),\n",
       " Timestamp('2023-03-14 15:15:00'),\n",
       " Timestamp('2023-03-14 15:00:00'),\n",
       " Timestamp('2023-03-14 14:45:00'),\n",
       " Timestamp('2023-03-14 14:30:00'),\n",
       " Timestamp('2023-03-14 14:15:00'),\n",
       " Timestamp('2023-03-14 14:00:00'),\n",
       " Timestamp('2023-03-14 13:45:00'),\n",
       " Timestamp('2023-03-14 13:30:00'),\n",
       " Timestamp('2023-03-14 13:15:00'),\n",
       " Timestamp('2023-03-14 13:00:00'),\n",
       " Timestamp('2023-03-14 12:45:00'),\n",
       " Timestamp('2023-03-14 12:30:00'),\n",
       " Timestamp('2023-03-14 12:15:00'),\n",
       " Timestamp('2023-03-14 12:00:00'),\n",
       " Timestamp('2023-03-14 11:45:00'),\n",
       " Timestamp('2023-03-14 11:30:00'),\n",
       " Timestamp('2023-03-14 11:15:00'),\n",
       " Timestamp('2023-03-14 11:00:00'),\n",
       " Timestamp('2023-03-14 10:45:00'),\n",
       " Timestamp('2023-03-14 10:30:00'),\n",
       " Timestamp('2023-03-14 10:15:00'),\n",
       " Timestamp('2023-03-14 10:00:00'),\n",
       " Timestamp('2023-03-14 09:45:00'),\n",
       " Timestamp('2023-03-14 09:30:00'),\n",
       " Timestamp('2023-03-14 09:15:00'),\n",
       " Timestamp('2023-03-14 09:00:00'),\n",
       " Timestamp('2023-03-14 08:45:00'),\n",
       " Timestamp('2023-03-14 08:30:00'),\n",
       " Timestamp('2023-03-14 07:30:00'),\n",
       " Timestamp('2023-03-13 20:00:00'),\n",
       " Timestamp('2023-03-13 19:30:00'),\n",
       " Timestamp('2023-03-13 19:00:00'),\n",
       " Timestamp('2023-03-13 18:15:00'),\n",
       " Timestamp('2023-03-13 17:30:00'),\n",
       " Timestamp('2023-03-13 17:00:00'),\n",
       " Timestamp('2023-03-13 16:30:00'),\n",
       " Timestamp('2023-03-13 16:15:00'),\n",
       " Timestamp('2023-03-13 16:00:00'),\n",
       " Timestamp('2023-03-13 15:45:00'),\n",
       " Timestamp('2023-03-13 15:30:00'),\n",
       " Timestamp('2023-03-13 15:15:00'),\n",
       " Timestamp('2023-03-13 15:00:00'),\n",
       " Timestamp('2023-03-13 14:45:00'),\n",
       " Timestamp('2023-03-13 14:30:00'),\n",
       " Timestamp('2023-03-13 14:15:00'),\n",
       " Timestamp('2023-03-13 14:00:00'),\n",
       " Timestamp('2023-03-13 13:45:00'),\n",
       " Timestamp('2023-03-13 13:30:00'),\n",
       " Timestamp('2023-03-13 13:15:00'),\n",
       " Timestamp('2023-03-13 13:00:00')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# url_link = (r\"https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol={}&interval=15min&apikey={}\".format('IBM', os.environ.get('Stock_API_key')))\n",
    "# url_data = request.urlopen(url_link)\n",
    "# raw_data = url_data.read()\n",
    "# json_data = json.loads(raw_data)\n",
    "\n",
    "# dates = pd.DataFrame(json_data['Time Series (15min)']).transpose().reset_index()\n",
    "# dates = pd.to_datetime(dates['index']).dt.date\n",
    "# dates = pd.DataFrame(json_data['Time Series (15min)'])\n",
    "# dates = pd.to_datetime(dates.transpose().reset_index()['index'])\n",
    "list(dates)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae52a0ac",
   "metadata": {},
   "source": [
    "Resources used\n",
    "\n",
    "https://sparkbyexamples.com/pandas/pandas-split-column/#:~:text=In%20Pandas%2C%20the%20apply(),to%20split%20into%20two%20columns.\n",
    "\n",
    "- Used when splitting date and post number uses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "17469b5663907bebcd1f31c6dfd445d1142c85a3d754cd7c9d1cfb642edcb7ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
